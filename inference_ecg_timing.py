#!/usr/bin/env python3
"""
ECG + Timing Features Inference Script

This script:
1. Loads a random ECG from the dataset
2. Extracts timing features using NeuroKit2
3. Passes combined input through trained sparse autoencoder
4. Shows which features activate most with clinical interpretations
5. Plots ECG with timing annotations and feature activations

Usage: python inference_ecg_timing.py [--model_path MODEL.pth]
"""

import argparse
import random
import json
from pathlib import Path
import numpy as np
import torch
import matplotlib.pyplot as plt

try:
    import neurokit2 as nk
except ImportError:
    raise SystemExit("Install NeuroKit2: pip install neurokit2")

from data_loader import PTBXLDataset
from sparse_autoencoder import GatedSparseAutoencoder
from tools.timing_extractor import get_timing_extractor


def load_feature_interpretations(filename: str = 'feature_interpretations.json') -> dict:
    """
    Load feature interpretations generated by analyze_features.py.
    
    Args:
        filename: Path to the feature interpretations JSON file
        
    Returns:
        Dictionary mapping feature_idx to interpretation data
    """
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            interpretations = json.load(f)
        
        # Convert to dict indexed by feature_idx for quick lookup
        feature_dict = {}
        for item in interpretations:
            feature_dict[item['feature_idx']] = item
            
        print(f"Loaded interpretations for {len(feature_dict)} features")
        return feature_dict
        
    except FileNotFoundError:
        print(f"Warning: Feature interpretations file '{filename}' not found.")
        print("Run analyze_features.py first to generate clinical interpretations.")
        return {}
    except Exception as e:
        print(f"Error loading feature interpretations: {e}")
        return {}


def extract_timing_features_with_fiducials(ecg_signal: np.ndarray, ecg_id: int = None, sampling_rate: int = 100) -> dict:
    """Extract timing features with fiducials using unified extractor."""
    try:
        # Use the same timing extractor as training (with cache enabled)
        extractor = get_timing_extractor(cache_path="times.csv", use_cache=True)
        
        # Use the same method as data loader - pass ECG ID if available
        features = extractor.get_timing_features(
            ecg_id=ecg_id,
            signal_2d=ecg_signal,
            sampling_rate=sampling_rate
        )
        
        # Also extract fiducials for visualization using NeuroKit2
        ecg_1d = ecg_signal[:, 1] if ecg_signal.ndim == 2 and ecg_signal.shape[1] > 1 else ecg_signal.flatten()
        
        signals, info = nk.ecg_process(ecg_1d, sampling_rate=sampling_rate)
        r_peaks = info["ECG_R_Peaks"]
        
        fiducials = {}
        if len(r_peaks) >= 2:
            # Delineate waves for fiducial points
            _, waves = nk.ecg_delineate(signals["ECG_Clean"], r_peaks, sampling_rate=sampling_rate)
            
            def get_points(key):
                pts = waves.get(key, [])
                return np.array(pts)[~np.isnan(pts)].astype(int) if pts is not None else np.array([])
            
            p_onsets = get_points("ECG_P_Onsets")
            q_peaks = get_points("ECG_Q_Peaks") 
            s_peaks = get_points("ECG_S_Peaks")
            t_offsets = get_points("ECG_T_Offsets")
            
            # Get fiducials for first beat
            r_peak = r_peaks[0]
            p_onset = p_onsets[p_onsets < r_peak][-1] if len(p_onsets[p_onsets < r_peak]) > 0 else None
            q_peak = q_peaks[np.abs(q_peaks - r_peak) <= 25][0] if len(q_peaks[np.abs(q_peaks - r_peak) <= 25]) > 0 else r_peak
            s_peak = s_peaks[np.abs(s_peaks - r_peak) <= 50][0] if len(s_peaks[np.abs(s_peaks - r_peak) <= 50]) > 0 else r_peak
            t_offset = t_offsets[t_offsets > r_peak][0] if len(t_offsets[t_offsets > r_peak]) > 0 else None
            
            fiducials = {
                'p_onset': p_onset,
                'q_peak': q_peak,
                'r_peak': r_peak,
                's_peak': s_peak,
                't_offset': t_offset
            }
        
        return {
            'features': features,
            'fiducials': fiducials,
            'success': True
        }
        
    except Exception as e:
        print(f"Timing extraction failed: {e}")
        return {
            'features': np.array([150.0, 80.0, 400.0, 70.0]),
            'fiducials': {},
            'success': False
        }


def plot_ecg_with_features(ecg_signal: np.ndarray, timing_result: dict,
                          feature_activations: np.ndarray, top_features: list,
                          report: str, feature_interpretations: dict = None,
                          sampling_rate: int = 100, save_path: str = None,
                          recon_topk: np.ndarray | None = None,
                          mse_topk: float | None = None):
    """Plot ECG + activations with an optional large interpretation text box BELOW the plots."""
    time_axis = np.arange(ecg_signal.shape[0]) / sampling_rate

    use_big_box = bool(top_features and feature_interpretations)
    if use_big_box:
        from matplotlib.gridspec import GridSpec
        fig = plt.figure(figsize=(14, 9))
        gs = GridSpec(3, 1, height_ratios=[1, 1, 1.4], hspace=0.32)
        ax1 = fig.add_subplot(gs[0])
        ax2 = fig.add_subplot(gs[1])
        axbox = fig.add_subplot(gs[2])
    else:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 7), sharex=False)
        axbox = None

    # ECG plot
    ax1.plot(time_axis, ecg_signal, 'b-', linewidth=0.8, label='Original ECG (Lead II)')
    if recon_topk is not None:
        ax1.plot(time_axis, recon_topk, 'r--', linewidth=1.0, label='Reconstruction (top-k)')
    if timing_result['success']:
        fiducials = timing_result['fiducials']
        colors = {'p_onset': 'green', 'q_peak': 'red', 'r_peak': 'darkred', 's_peak': 'red', 't_offset': 'orange'}
        for name, sample_idx in fiducials.items():
            if sample_idx is not None:
                t_s = sample_idx / sampling_rate
                color = colors.get(name, 'purple')
                ax1.axvline(t_s, color=color, linestyle='--', alpha=0.7)
                ax1.plot(t_s, ecg_signal[sample_idx], 'o', color=color, markersize=5, label=name.upper())
        # Shaded intervals
        p_on = fiducials.get('p_onset'); q_pk = fiducials.get('q_peak'); s_pk = fiducials.get('s_peak'); t_off = fiducials.get('t_offset')
        if p_on is not None and q_pk is not None:
            ax1.axvspan(p_on / sampling_rate, q_pk / sampling_rate, alpha=0.2, color='green', label='PR')
        if q_pk is not None and s_pk is not None:
            ax1.axvspan(q_pk / sampling_rate, s_pk / sampling_rate, alpha=0.2, color='red', label='QRS')
        if q_pk is not None and t_off is not None:
            ax1.axvspan(q_pk / sampling_rate, t_off / sampling_rate, alpha=0.1, color='orange', label='QT')
    ax1.set_xlabel('Time (s)')
    ax1.set_ylabel('Amplitude (mV)')
    title_extra = f"  (Top-k MSE: {mse_topk:.4f})" if mse_topk is not None else ""
    ax1.set_title(f'ECG with Timing Intervals{title_extra}')
    ax1.grid(True, alpha=0.3)
    ax1.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)

    # Activation bar plot
    ax2.bar(range(len(feature_activations)), feature_activations, alpha=0.7)
    ax2.set_xlabel('Feature Index')
    ax2.set_ylabel('Activation Strength')
    ax2.set_title('Sparse Autoencoder Feature Activations')
    ax2.grid(True, alpha=0.3)
    for feat_idx, activation in top_features:
        ax2.bar(feat_idx, activation, color='red', alpha=0.85)
        ax2.text(feat_idx, activation + 0.05, f'F{feat_idx}', ha='center', va='bottom', fontsize=8, fontweight='bold')

    # Big interpretation box below
    if use_big_box and axbox is not None:
        axbox.axis('off')
        interp_lines = []
        for feat_idx, activation in top_features:
            interp = feature_interpretations.get(feat_idx, {})
            key_word = interp.get('key_word', 'N/A')
            summary = interp.get('summary', '').strip()
            interp_lines.append(f"Feature {feat_idx} ({key_word})")
            if summary:
                interp_lines.append(summary)
            # Ensure exactly one blank line after each feature block
            if not interp_lines or interp_lines[-1] != "":
                interp_lines.append("")
        # Trim excess
        interp_text = '\n'.join(interp_lines[:120])
        import textwrap
        wrapped = '\n'.join(textwrap.wrap(interp_text, width=150, break_long_words=False))
        axbox.text(0, 1, wrapped, ha='left', va='top', fontsize=9, fontfamily='monospace', linespacing=1.25)
    else:
        fig.tight_layout()

    if save_path:
        plots_dir = Path('plots'); plots_dir.mkdir(exist_ok=True)
        plt.savefig(plots_dir / save_path, dpi=300, bbox_inches='tight')
        print(f"Figure saved to plots/{save_path}")
    plt.show()


def main():
    parser = argparse.ArgumentParser(description="ECG + Timing Features Inference")
    parser.add_argument("--model_path", default="checkpoints/best_model_fixed.pth",
                       help="Path to trained model")
    parser.add_argument("--data_path", default="physionet.org/files/ptb-xl/1.0.3/",
                       help="PTB-XL dataset path")
    parser.add_argument("--n_samples", type=int, default=3,
                       help="Number of random samples to analyze")
    parser.add_argument("--top_k", type=int, default=5,
                       help="Number of top features to highlight")
    parser.add_argument("--interpretations", default="feature_interpretations.json",
                       help="Path to feature interpretations JSON file")
    parser.add_argument("--mi_only", action="store_true",
                       help="Filter to ECGs whose report mentions 'myocardial infarction'")
    parser.add_argument("--keyword", action="append",
                       help="Additional keyword to filter reports by (case-insensitive). Can be used multiple times.")
    parser.add_argument("--max_samples", type=int, default=1000,
                       help="Limit number of samples loaded from dataset (default: all)")
    
    args = parser.parse_args()
    
    args.top_k_reconstruction = None  # Use ALL features for reconstruction
    # Load dataset
    dataset = PTBXLDataset(args.data_path, sampling_rate=100, normalize=True, max_samples=args.max_samples)
    print(f"Loaded {len(dataset)} ECG samples")

    # Build filter keyword list
    filter_keywords = []
    if args.mi_only:
        filter_keywords.append("myocardial infarction")
        # Add common shorthand variants
        filter_keywords.extend(["stemi", "nstemi", "infarct", "infarction"])
    if args.keyword:
        filter_keywords.extend([k.lower() for k in args.keyword])

    # Determine candidate indices based on keywords
    if filter_keywords:
        unique_keywords = sorted(set(filter_keywords))
        print(f"Filtering reports by keywords: {unique_keywords}")
        candidate_indices = []
        matched_snippets = []
        for i in range(len(dataset)):
            report_raw = str(dataset.metadata.iloc[i].get('report', ''))
            report_text = report_raw.lower()
            if any(kw in report_text for kw in unique_keywords):
                candidate_indices.append(i)
                snippet = (report_raw[:140] + '...') if len(report_raw) > 140 else report_raw
                matched_snippets.append(snippet)
        if not candidate_indices:
            print("Warning: No ECG reports matched the provided keywords. Using full dataset instead.")
            candidate_indices = list(range(len(dataset)))
        else:
            print(f"Found {len(candidate_indices)} ECGs matching keywords (out of {len(dataset)} total)")
            print("Showing up to first 5 matched report snippets:")
            for snip in matched_snippets[:5]:
                print("  -", snip.replace('\n', ' ') )
    else:
        candidate_indices = list(range(len(dataset)))
    
    # Load feature interpretations
    feature_interpretations = load_feature_interpretations(args.interpretations)
    
    # Load the trained model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Load checkpoint with safe globals for PyTorch 2.6 compatibility
    with torch.serialization.safe_globals([
        np._core.multiarray.scalar, 
        np.dtype, 
        np.ndarray,
        np.float32,
        np.float64,
        np.int32,
        np.int64,
        np.dtypes.Int64DType,
        np.dtypes.Float64DType,
        np.dtypes.Int32DType,
        np.dtypes.Float32DType
    ]):
        checkpoint = torch.load(args.model_path, map_location='cpu')
    
    # Extract model configuration
    config = checkpoint.get('model_config', {})
    
    # Calculate ECG and timing dimensions from total input dimension
    total_input_dim = config.get('input_dim', 12004)  # 12000 ECG + 4 timing features
    ecg_input_dim = total_input_dim - 4
    timing_features_dim = 4
    
    # Import here to avoid circular import
    from sparse_autoencoder import GatedSparseAutoencoder
    
    # Create model with loaded configuration
    model = GatedSparseAutoencoder(
        ecg_input_dim=ecg_input_dim,
        timing_features_dim=timing_features_dim,
        hidden_dims=config.get('hidden_dims', [2048, 1024, 512]),
        latent_dim=config.get('latent_dim', 256),
        sparsity_weight=config.get('sparsity_weight', 0.01),
        alpha_aux=config.get('alpha_aux', 0.02),
        target_sparsity=config.get('target_sparsity', 0.5),
        use_frozen_decoder_for_aux=config.get('use_frozen_decoder_for_aux', True)
    )
    
    # Load model state
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    model.to(device)
    
    print(f"Model loaded, using device: {device}")
    
    # Analyze random samples
    random.seed(42)
    # Sample from candidate indices
    indices = random.sample(candidate_indices, min(args.n_samples, len(candidate_indices)))
    
    for i, idx in enumerate(indices):
        sample = dataset[idx]
        row_meta = dataset.metadata.iloc[sample['idx']]
        
        print(f"\\n[{i+1}/{args.n_samples}] ECG ID: {row_meta.name}")
        
        # Get ECG signal
        ecg_signal_2d = sample['signal'].numpy()
        ecg_signal_flat = sample['signal_flat'].numpy()
        
        # Extract timing features
        timing_result = extract_timing_features_with_fiducials(ecg_signal_2d, ecg_id=row_meta.name, sampling_rate=100)
        timing_features = timing_result['features']
        
        print(f"Timing features: PR={timing_features[0]:.0f}, QRS={timing_features[1]:.0f}, "
              f"QT={timing_features[2]:.0f}, HR={timing_features[3]:.0f}")
        
        # Combine ECG and timing features
        combined_input = np.concatenate([ecg_signal_flat, timing_features])
        combined_tensor = torch.FloatTensor(combined_input).unsqueeze(0).to(device)
        
        # Forward pass through model
        with torch.no_grad():
            full_recon, latent = model(combined_tensor)
            feature_activations = latent.squeeze(0).cpu().numpy()

        # Top features for highlighting
        top_indices = np.argsort(np.abs(feature_activations))[-args.top_k:][::-1]
        top_features = [(idx, feature_activations[idx]) for idx in top_indices]

        # Separate k for reconstruction  
        k_recon = args.top_k_reconstruction if args.top_k_reconstruction is not None else len(feature_activations)
        k_recon = max(1, min(k_recon, feature_activations.shape[0]))
        recon_indices = np.argsort(np.abs(feature_activations))[-k_recon:][::-1]

        print(f"Top {args.top_k} activating features:")
        for feat_idx, activation in top_features:
            interpretation_info = ""
            if feat_idx in feature_interpretations:
                interp = feature_interpretations[feat_idx]
                key_word = interp.get('key_word', '')
                summary = interp.get('summary', '')
                interpretation_info = f" | {key_word} | {summary[:60]}{'...' if len(summary) > 60 else ''}"
            print(f"  Feature {feat_idx}: {activation:.4f}{interpretation_info}\n")

        # Get report
        report = str(row_meta.get('report', '')).strip()
        print(f"Clinical report: {report}")

        # Reconstruct using all features (no top-k filtering)
        with torch.no_grad():
            if args.top_k_reconstruction is None:
                # Use all features without filtering
                recon_full_input = model.decode(latent, use_frozen=False)
                print(f"Using ALL {latent.shape[1]} features for reconstruction")
            else:
                # Use top-k features
                latent_subset = latent.clone()
                mask = torch.zeros_like(latent_subset)
                recon_indices_tensor = torch.tensor(list(recon_indices), device=latent_subset.device)
                mask[0, recon_indices_tensor] = 1.0
                latent_subset = latent_subset * mask
                recon_full_input = model.decode(latent_subset, use_frozen=False)
                print(f"Using top {k_recon} features for reconstruction")
            recon_np = recon_full_input.squeeze(0).cpu().numpy()

        recon_ecg_flat = recon_np[:ecg_signal_flat.shape[0]]
        ecg_1d = ecg_signal_2d[:, 1] if ecg_signal_2d.shape[1] > 1 else ecg_signal_2d[:, 0]
        n_time, n_leads = ecg_signal_2d.shape
        recon_ecg_matrix = recon_ecg_flat.reshape(n_time, n_leads)
        recon_lead = recon_ecg_matrix[:, 1] if n_leads > 1 else recon_ecg_matrix[:, 0]
        mse_topk = float(np.mean((ecg_1d - recon_lead)**2))
        print(f"Reconstruction using top {k_recon} features MSE: {mse_topk:.5f}")

        # Plot
        save_filename = f"ecg_analysis_{row_meta.name}_{i+1}.png"
        plot_ecg_with_features(ecg_1d, timing_result, feature_activations,
                               top_features, report, feature_interpretations,
                               sampling_rate=100, save_path=save_filename,
                               recon_topk=recon_lead, mse_topk=mse_topk)

        print("-" * 60)


if __name__ == "__main__":
    main()
