#!/usr/bin/env python3
"""
Simplified ECG Heartbeats Inference Script

This script:
1. Loads a random ECG from the dataset
2. Extracts 3 representative heartbeats from lead II (750ms each, centered 100ms after QRS)
3. Passes heartbeats through trained sparse autoencoder
4. Shows which features activate most with clinical interpretations
5. Plots the 3 heartbeats and their reconstructions alongside feature activations

Usage: python inference_ecg_timing.py [--model_path MODEL.pth]
"""

import argparse
import random
import json
from pathlib import Path
import numpy as np
import torch
import matplotlib.pyplot as plt

try:
    import neurokit2 as nk
except ImportError:
    raise SystemExit("Install NeuroKit2: pip install neurokit2")

from data_loader import PTBXLDataset
from preprocess_data import extract_representative_heartbeats
from sparse_autoencoder import GatedSparseAutoencoder
from simple_autoencoder import SimpleAutoencoder


def detect_model_type(checkpoint):
    """
    Detect whether the checkpoint is from a GatedSparseAutoencoder or SimpleAutoencoder.
    
    Args:
        checkpoint: The loaded checkpoint dictionary
        
    Returns:
        str: 'sparse' for GatedSparseAutoencoder, 'simple' for SimpleAutoencoder
    """
    state_dict = checkpoint.get('model_state_dict', {})
    
    # Check for GatedSparseAutoencoder specific parameters
    sparse_keys = ['W_gate', 'b_gate', 'r_mag', 'b_mag', 'W_dec', 'b_dec']
    if any(key in state_dict for key in sparse_keys):
        return 'sparse'
    
    # Check for SimpleAutoencoder specific parameters
    simple_keys = ['encoder.0.weight', 'decoder.0.weight']
    if any(key in state_dict for key in simple_keys):
        return 'simple'
    
    # Default fallback (could also raise an error)
    print("Warning: Could not detect model type, defaulting to sparse")
    return 'sparse'


def create_model_from_checkpoint(checkpoint, heartbeat_input_dim, config):
    """
    Create the appropriate model based on the checkpoint type.
    
    Args:
        checkpoint: The loaded checkpoint
        heartbeat_input_dim: Input dimension for heartbeats
        config: Configuration dictionary
        
    Returns:
        The created model instance
    """
    model_type = detect_model_type(checkpoint)
    
    model_args = {
        'heartbeat_input_dim': heartbeat_input_dim,
        'hidden_dims': config.get('hidden_dims', [512, 256]),
        'latent_dim': config.get('latent_dim', 128),
        'sparsity_weight': config.get('sparsity_weight', 0.01),
        'alpha_aux': config.get('alpha_aux', 0.02),
        'target_sparsity': config.get('target_sparsity', 0.1),
        'use_frozen_decoder_for_aux': config.get('use_frozen_decoder_for_aux', True)
    }
    
    if model_type == 'simple':
        print("Loading SimpleAutoencoder model...")
        model = SimpleAutoencoder(**model_args)
    else:
        print("Loading GatedSparseAutoencoder model...")
        model = GatedSparseAutoencoder(**model_args)
    
    return model


def load_feature_interpretations(filename: str = 'feature_interpretations.json') -> dict:
    """
    Load feature interpretations generated by analyze_features.py.
    
    Args:
        filename: Path to the feature interpretations JSON file
        
    Returns:
        Dictionary mapping feature_idx to interpretation data
    """
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            interpretations = json.load(f)
        
        # Convert to dict indexed by feature_idx for quick lookup
        feature_dict = {}
        for item in interpretations:
            feature_dict[item['feature_idx']] = item
            
        print(f"Loaded interpretations for {len(feature_dict)} features")
        return feature_dict
        
    except FileNotFoundError:
        print(f"Warning: Feature interpretations file '{filename}' not found.")
        print("Run analyze_features.py first to generate clinical interpretations.")
        return {}
    except Exception as e:
        print(f"Error loading feature interpretations: {e}")
        return {}


def extract_heartbeats_with_timing(ecg_signal: np.ndarray, sampling_rate: int = 100) -> dict:
    """Extract 3 representative heartbeats and basic timing info for visualization."""
    try:
        # Extract heartbeats using the same function as the data loader
        heartbeats = extract_representative_heartbeats(
            ecg_signal,
            sampling_rate=sampling_rate,
            lead_idx=1,  # Lead II
            num_beats=3,
            beat_duration_ms=750,
            qrs_offset_ms=100
        )
        
        # Also extract R peaks for basic visualization
        ecg_1d = ecg_signal[:, 1] if ecg_signal.ndim == 2 and ecg_signal.shape[1] > 1 else ecg_signal.flatten()
        signals, info = nk.ecg_process(ecg_1d, sampling_rate=sampling_rate)
        r_peaks = info["ECG_R_Peaks"]
        
        # Calculate beat duration in samples
        beat_samples = int(750 * sampling_rate / 1000)  # 750ms
        
        return {
            'heartbeats': heartbeats,
            'beat_samples': beat_samples,
            'r_peaks': r_peaks[:3] if len(r_peaks) >= 3 else r_peaks,
            'ecg_clean': ecg_1d,
            'success': True
        }
        
    except Exception as e:
        print(f"Heartbeat extraction failed: {e}")
        # Return zeros as fallback
        beat_samples = int(750 * sampling_rate / 1000)
        return {
            'heartbeats': np.zeros(3 * beat_samples),
            'beat_samples': beat_samples,
            'r_peaks': [],
            'ecg_clean': ecg_signal[:, 1] if ecg_signal.ndim == 2 else ecg_signal.flatten(),
            'success': False
        }


def plot_heartbeats_with_features(heartbeats_data: dict, feature_activations: np.ndarray, 
                                 top_features: list, report: str, 
                                 feature_interpretations: dict = None,
                                 sampling_rate: int = 100, save_path: str = None,
                                 reconstructed_heartbeats: np.ndarray = None,
                                 sparse_reconstructed_heartbeats: np.ndarray = None,
                                 full_mse: float = None,
                                 sparse_mse: float = None):
    """Plot 3 heartbeats and their reconstructions with feature activations."""
    
    heartbeats = heartbeats_data['heartbeats']
    beat_samples = heartbeats_data['beat_samples']
    
    # Split heartbeats into individual beats
    beat1 = heartbeats[:beat_samples]
    beat2 = heartbeats[beat_samples:2*beat_samples]
    beat3 = heartbeats[2*beat_samples:3*beat_samples]
    
    # Split reconstructed heartbeats if provided
    recon_beats = []
    sparse_recon_beats = []
    if reconstructed_heartbeats is not None:
        recon_beats = [
            reconstructed_heartbeats[:beat_samples],
            reconstructed_heartbeats[beat_samples:2*beat_samples],
            reconstructed_heartbeats[2*beat_samples:3*beat_samples]
        ]
    
    if sparse_reconstructed_heartbeats is not None:
        sparse_recon_beats = [
            sparse_reconstructed_heartbeats[:beat_samples],
            sparse_reconstructed_heartbeats[beat_samples:2*beat_samples],
            sparse_reconstructed_heartbeats[2*beat_samples:3*beat_samples]
        ]
    
    # Time axis for each beat (750ms)
    beat_time = np.arange(beat_samples) * 1000 / sampling_rate  # in milliseconds
    
    use_interpretations = bool(top_features and feature_interpretations)
    if use_interpretations:
        from matplotlib.gridspec import GridSpec
        fig = plt.figure(figsize=(16, 14))
        gs = GridSpec(5, 3, height_ratios=[1, 1, 0.8, 0.8, 1.2], width_ratios=[1, 1, 1], 
                     hspace=0.35, wspace=0.3)
        
        # Heartbeat plots (top row)
        ax1 = fig.add_subplot(gs[0, 0])
        ax2 = fig.add_subplot(gs[0, 1])
        ax3 = fig.add_subplot(gs[0, 2])
        
        # Activation plot (second row, spanning all columns)
        ax_act = fig.add_subplot(gs[1, :])
        
        # Full reconstruction comparison (third row, spanning all columns)
        ax_recon = fig.add_subplot(gs[2, :])
        
        # Sparse reconstruction comparison (fourth row, spanning all columns)
        ax_sparse_recon = fig.add_subplot(gs[3, :])
        
        # Interpretation box (bottom row, spanning all columns)
        ax_interp = fig.add_subplot(gs[4, :])
    else:
        fig = plt.figure(figsize=(16, 8))
        gs = GridSpec(3, 3, height_ratios=[1, 1, 0.8], width_ratios=[1, 1, 1], 
                     hspace=0.3, wspace=0.3)
        
        # Heartbeat plots
        ax1 = fig.add_subplot(gs[0, 0])
        ax2 = fig.add_subplot(gs[0, 1])
        ax3 = fig.add_subplot(gs[0, 2])
        
        # Activation plot
        ax_act = fig.add_subplot(gs[1, :])
        
        # Reconstruction comparison
        ax_recon = fig.add_subplot(gs[2, :])
        
        ax_interp = None
    
    # Plot individual heartbeats
    beats = [beat1, beat2, beat3]
    axes = [ax1, ax2, ax3]
    
    for i, (beat, ax) in enumerate(zip(beats, axes)):
        ax.plot(beat_time, beat, 'b-', linewidth=1.2, label='Original')
        if recon_beats:
            ax.plot(beat_time, recon_beats[i], 'r--', linewidth=1.2, label='Reconstruction')
        
        # Mark QRS center (100ms offset point)
        qrs_center_ms = 100 + (750 / 2)  # QRS offset + half beat duration
        if qrs_center_ms <= beat_time[-1]:
            qrs_idx = int(qrs_center_ms * sampling_rate / 1000)
            if qrs_idx < len(beat):
                ax.axvline(qrs_center_ms, color='red', linestyle=':', alpha=0.7, label='QRS+100ms')
                ax.plot(qrs_center_ms, beat[qrs_idx], 'ro', markersize=4)
        
        ax.set_title(f'Heartbeat {i+1}')
        ax.set_xlabel('Time (ms)')
        ax.set_ylabel('Amplitude')
        ax.grid(True, alpha=0.3)
        if i == 0:  # Only show legend on first plot
            ax.legend(fontsize=8)
    
    # Feature activation plot
    ax_act.bar(range(len(feature_activations)), feature_activations, alpha=0.7, color='lightblue')
    ax_act.set_xlabel('Feature Index')
    ax_act.set_ylabel('Activation Strength')
    ax_act.set_title('Sparse Autoencoder Feature Activations')
    ax_act.grid(True, alpha=0.3)
    
    # Highlight top features
    for feat_idx, activation in top_features:
        ax_act.bar(feat_idx, activation, color='red', alpha=0.85)
        ax_act.text(feat_idx, activation + max(feature_activations) * 0.05, 
                   f'F{feat_idx}', ha='center', va='bottom', fontsize=8, fontweight='bold')
    
    # Reconstruction comparison plot
    if reconstructed_heartbeats is not None:
        full_time = np.arange(len(heartbeats)) * 1000 / sampling_rate
        ax_recon.plot(full_time, heartbeats, 'b-', linewidth=1.0, label='Original Heartbeats', alpha=0.8)
        ax_recon.plot(full_time, reconstructed_heartbeats, 'r--', linewidth=1.0, label='Reconstructed', alpha=0.8)
        
        # Mark beat boundaries
        for i in range(1, 3):
            boundary_ms = i * 750
            ax_recon.axvline(boundary_ms, color='gray', linestyle=':', alpha=0.5)
            ax_recon.text(boundary_ms, max(heartbeats) * 0.9, f'Beat {i+1}', 
                         rotation=90, va='top', ha='right', fontsize=8, alpha=0.7)
        
        mse_text = f"MSE: {full_mse:.5f}" if full_mse is not None else ""
        ax_recon.set_title(f'Full Reconstruction (All Features)  {mse_text}')
        ax_recon.set_xlabel('Time (ms)')
        ax_recon.set_ylabel('Amplitude')
        ax_recon.legend()
        ax_recon.grid(True, alpha=0.3)
    
    # Plot sparse reconstruction (top 5 features only)
    if sparse_reconstructed_heartbeats is not None:
        full_time = np.arange(len(heartbeats)) * 1000 / sampling_rate
        ax_sparse_recon.plot(full_time, heartbeats, 'b-', linewidth=1.0, label='Original Heartbeats', alpha=0.8)
        ax_sparse_recon.plot(full_time, sparse_reconstructed_heartbeats, 'g--', linewidth=1.5, label='Top-5 Features Reconstruction', alpha=0.9)
        
        # Mark beat boundaries
        for i in range(1, 3):
            boundary_ms = i * 750
            ax_sparse_recon.axvline(boundary_ms, color='gray', linestyle=':', alpha=0.5)
            ax_sparse_recon.text(boundary_ms, max(heartbeats) * 0.9, f'Beat {i+1}', 
                                rotation=90, va='top', ha='right', fontsize=8, alpha=0.7)
        
        sparse_mse_text = f"MSE: {sparse_mse:.5f}" if sparse_mse is not None else ""
        ax_sparse_recon.set_title(f'Sparse Reconstruction (Top-5 Features Only)  {sparse_mse_text}')
        ax_sparse_recon.set_xlabel('Time (ms)')
        ax_sparse_recon.set_ylabel('Amplitude')
        ax_sparse_recon.legend()
        ax_sparse_recon.grid(True, alpha=0.3)
    
    # Feature interpretations
    if use_interpretations and ax_interp is not None:
        ax_interp.axis('off')
        interp_lines = []
        
        for feat_idx, activation in top_features:
            interp = feature_interpretations.get(feat_idx, {})
            key_word = interp.get('key_word', 'N/A')
            summary = interp.get('summary', '').strip()
            
            interp_lines.append(f"Feature {feat_idx} (activation: {activation:.4f}) - {key_word}")
            if summary:
                interp_lines.append(f"  {summary}")
            interp_lines.append("")  # Blank line between features
        
        # Add clinical report
        interp_lines.append(f"Clinical Report:")
        interp_lines.append(f"  {report}")
        
        interp_text = '\n'.join(interp_lines)
        import textwrap
        wrapped = '\n'.join(textwrap.wrap(interp_text, width=120, break_long_words=False))
        ax_interp.text(0.05, 0.95, wrapped, ha='left', va='top', fontsize=9, 
                      fontfamily='monospace', transform=ax_interp.transAxes)
    
    plt.tight_layout()
    
    if save_path:
        plots_dir = Path('plots')
        plots_dir.mkdir(exist_ok=True)
        plt.savefig(plots_dir / save_path, dpi=300, bbox_inches='tight')
        print(f"Figure saved to plots/{save_path}")
    
    plt.show()


def main():
    parser = argparse.ArgumentParser(description="Simplified ECG Heartbeats Inference")
    parser.add_argument("--model_path", default="checkpoints/best_model.pth",
                       help="Path to trained model")
    parser.add_argument("--data_path", default="physionet.org/files/ptb-xl/1.0.3/",
                       help="PTB-XL dataset path")
    parser.add_argument("--preprocessed_path", default="preprocessed_data",
                       help="Path to preprocessed data directory")
    parser.add_argument("--n_samples", type=int, default=3,
                       help="Number of random samples to analyze")
    parser.add_argument("--top_k", type=int, default=5,
                       help="Number of top features to highlight")
    parser.add_argument("--interpretations", default="feature_interpretations.json",
                       help="Path to feature interpretations JSON file")
    parser.add_argument("--mi_only", action="store_true",
                       help="Filter to ECGs whose report mentions 'myocardial infarction'")
    parser.add_argument("--keyword", action="append",
                       help="Additional keyword to filter reports by (case-insensitive). Can be used multiple times.")
    parser.add_argument("--max_samples", type=int, default=1000,
                       help="Limit number of samples loaded from dataset (default: all)")
    
    args = parser.parse_args()
    
    args.top_k_reconstruction = None  # Use ALL features for reconstruction
    # Load dataset from preprocessed data
    dataset = PTBXLDataset(args.preprocessed_path, original_data_path=args.data_path, max_samples=args.max_samples)
    print(f"Loaded {len(dataset)} ECG samples")

    # Build filter keyword list
    filter_keywords = []
    if args.mi_only:
        filter_keywords.append("myocardial infarction")
        # Add common shorthand variants
        filter_keywords.extend(["stemi", "nstemi", "infarct", "infarction"])
    if args.keyword:
        filter_keywords.extend([k.lower() for k in args.keyword])

    # Determine candidate indices based on keywords
    if filter_keywords:
        unique_keywords = sorted(set(filter_keywords))
        print(f"Filtering reports by keywords: {unique_keywords}")
        candidate_indices = []
        matched_snippets = []
        for i in range(len(dataset)):
            report_raw = str(dataset.metadata.iloc[i].get('report', ''))
            report_text = report_raw.lower()
            if any(kw in report_text for kw in unique_keywords):
                candidate_indices.append(i)
                snippet = (report_raw[:140] + '...') if len(report_raw) > 140 else report_raw
                matched_snippets.append(snippet)
        if not candidate_indices:
            print("Warning: No ECG reports matched the provided keywords. Using full dataset instead.")
            candidate_indices = list(range(len(dataset)))
        else:
            print(f"Found {len(candidate_indices)} ECGs matching keywords (out of {len(dataset)} total)")
            print("Showing up to first 5 matched report snippets:")
            for snip in matched_snippets[:5]:
                print("  -", snip.replace('\n', ' ') )
    else:
        candidate_indices = list(range(len(dataset)))
    
    # Load feature interpretations
    feature_interpretations = load_feature_interpretations(args.interpretations)
    
    # Load the trained model
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # Load checkpoint with safe globals for PyTorch 2.6 compatibility
    with torch.serialization.safe_globals([
        np._core.multiarray.scalar, 
        np.dtype, 
        np.ndarray,
        np.float32,
        np.float64,
        np.int32,
        np.int64,
        np.dtypes.Int64DType,
        np.dtypes.Float64DType,
        np.dtypes.Int32DType,
        np.dtypes.Float32DType
    ]):
        checkpoint = torch.load(args.model_path, map_location='cpu')
    
    # Extract model configuration
    config = checkpoint.get('model_config', {})
    
    # Get heartbeat input dimension (3 beats * 750ms * sampling_rate / 1000)
    heartbeat_input_dim = config.get('heartbeat_input_dim', 225)  # Default for 100Hz
    
    # Create appropriate model based on checkpoint type
    model = create_model_from_checkpoint(checkpoint, heartbeat_input_dim, config)
    
    # Load model state
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    model.to(device)
    
    print(f"Model loaded, using device: {device}")
    
    # Analyze random samples
    random.seed(42)
    # Sample from candidate indices
    indices = random.sample(candidate_indices, min(args.n_samples, len(candidate_indices)))
    
    for i, idx in enumerate(indices):
        sample = dataset[idx]
        row_meta = dataset.metadata.iloc[sample['idx']]
        
        print(f"\\n[{i+1}/{args.n_samples}] ECG ID: {row_meta.name}")
        
        # Get ECG signal and extract heartbeats
        ecg_signal_2d = sample['signal'].numpy()
        
        # Extract heartbeats using the new function
        heartbeats_data = extract_heartbeats_with_timing(ecg_signal_2d, sampling_rate=100)
        
        if heartbeats_data['success']:
            print(f"Successfully extracted {3} heartbeats from Lead II")
            if heartbeats_data['r_peaks'] is not None and len(heartbeats_data['r_peaks']) > 0:
                avg_hr = 60 / (np.mean(np.diff(heartbeats_data['r_peaks'])) / 100) if len(heartbeats_data['r_peaks']) > 1 else 70
                print(f"Estimated heart rate: {avg_hr:.0f} bpm")
        else:
            print("Warning: Heartbeat extraction failed, using fallback data")
        
        # Use heartbeats from the sample (as processed by dataset)
        heartbeats = sample['heartbeats'].numpy()
        heartbeats_tensor = torch.FloatTensor(heartbeats).unsqueeze(0).to(device)
        
        # Forward pass through model
        with torch.no_grad():
            recon_heartbeats, latent = model(heartbeats_tensor)
            feature_activations = latent.squeeze(0).cpu().numpy()
            
            # Create sparse reconstruction using only top 5 features
            sparse_latent = torch.zeros_like(latent)
            top_5_indices = np.argsort(np.abs(feature_activations))[-5:]
            
            # Keep only top 5 features
            for idx in top_5_indices:
                sparse_latent[0, idx] = latent[0, idx]
            
            # Reconstruct from sparse latent representation
            sparse_recon_heartbeats = model.decode(sparse_latent, use_frozen=False)

        # Top features for highlighting
        top_indices = np.argsort(np.abs(feature_activations))[-args.top_k:][::-1]
        top_features = [(idx, feature_activations[idx]) for idx in top_indices]

        print(f"Top {args.top_k} activating features:")
        for feat_idx, activation in top_features:
            interpretation_info = ""
            if feat_idx in feature_interpretations:
                interp = feature_interpretations[feat_idx]
                key_word = interp.get('key_word', '')
                summary = interp.get('summary', '')
                interpretation_info = f" | {key_word} | {summary[:60]}{'...' if len(summary) > 60 else ''}"
            print(f"  Feature {feat_idx}: {activation:.4f}{interpretation_info}")

        # Get report
        report = str(row_meta.get('report', '')).strip()
        print(f"Clinical report: {report}")

        # Calculate reconstruction MSE
        recon_heartbeats_np = recon_heartbeats.squeeze(0).cpu().numpy()
        sparse_recon_heartbeats_np = sparse_recon_heartbeats.squeeze(0).cpu().numpy()
        
        full_mse = float(np.mean((heartbeats - recon_heartbeats_np)**2))
        sparse_mse = float(np.mean((heartbeats - sparse_recon_heartbeats_np)**2))
        
        print(f"Full reconstruction MSE: {full_mse:.6f}")
        print(f"Top-5 features reconstruction MSE: {sparse_mse:.6f}")
        print(f"Top-5 features used: {sorted(top_5_indices)}")

        # Plot
        save_filename = f"heartbeat_analysis_{row_meta.name}_{i+1}.png"
        plot_heartbeats_with_features(
            heartbeats_data, 
            feature_activations,
            top_features, 
            report, 
            feature_interpretations,
            sampling_rate=100, 
            save_path=save_filename,
            reconstructed_heartbeats=recon_heartbeats_np,
            sparse_reconstructed_heartbeats=sparse_recon_heartbeats_np,
            full_mse=full_mse,
            sparse_mse=sparse_mse
        )

        print("-" * 60)


if __name__ == "__main__":
    main()
